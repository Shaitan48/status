Техническое задание на компоненты системы Status Monitor
1. Online-агент (PowerShell)
Назначение: Online-агент устанавливается на узлах, имеющих прямой доступ к центральному серверу Status Monitor. Он предназначен для регулярного выполнения проверок (мониторинговых заданий) на локальном узле и немедленной отправки результатов на сервер через REST API. Online-агент использует модуль StatusMonitorAgentUtils для выполнения конкретных проверок (Ping, проверка дискового пространства, состояние сервисов и др.) и обеспечивает актуальный мониторинг узла в режиме реального времени. Входные данные: Online-агент читает свою локальную конфигурацию из файла config.json. В конфигурации задаются:
object_id – идентификатор узла (или объекта мониторинга) в системе, для которого агент будет получать задания.
apiBaseUrl – базовый URL API центрального сервера (например, http://server:48030/api).
api_key – API-ключ аутентификации с ролью agent, выданный сервером (позволяет агенту обращаться к защищённым эндпоинтам).
api_poll_interval_seconds – период опроса сервера (в секундах) для получения новых заданий.
default_check_interval_seconds – интервал по умолчанию между запусками отдельных проверок (если не указан индивидуально в задании).
Другие настройки: пути к лог-файлу (logFile), уровень логирования и пр.
Кроме файла настроек, входными данными фактически являются список заданий, которые агент запрашивает у серверного API. Эти задания содержат параметры проверок:
Идентификатор задания (assignment_id).
Тип проверки (например, метод PING, DISK_USAGE и т.д.).
Целевой узел или ресурс (может быть сам локальный хост или подсистема, IP-адрес для пинга, диск для проверки и т.д.).
Параметры проверки (parameters в виде JSON – например, процент свободного места, имя службы и пр.).
Желаемый интервал выполнения (interval_seconds), если переопределён для конкретного задания.
Критерии успеха (success_criteria – условия при которых результат считается успешным, например пороговое значение).
Online-агент получает этот список заданий динамически через API, поэтому напрямую в файл конфигурации задания не закладываются – они хранятся на сервере. Обработка: После запуска Online-агент выполняет следующие шаги:
Инициализация: Агент загружает конфигурацию (config.json), извлекая идентификатор объекта и настройки API. Затем импортирует модуль StatusMonitorAgentUtils, который предоставляет функцию Invoke-StatusMonitorCheck и набор скриптов Check-* для разных типов проверок.
Опрос сервера: В бесконечном цикле (или по расписанию, заданному через api_poll_interval_seconds) агент обращается к серверу для получения актуальных заданий. Для этого выполняется запрос типа GET к API, например: GET /api/v1/assignments?object_id=<ID> (где <ID> – object_id из конфигурации агента). В запросе передаётся API-ключ агента. Сервер аутентифицирует агента и возвращает JSON с массивом заданий, назначенных данному узлу.
Выполнение проверок: Получив список заданий (assignments), агент перебирает каждое из них и локально выполняет соответствующую проверку. Внутренне используется вызов Invoke-StatusMonitorCheck (из модуля StatusMonitorAgentUtils), которому передаются параметры задания:
На основании поля method_name или ID метода в задании, Invoke-StatusMonitorCheck выбирает скрипт проверки (например, Check-PING.ps1, Check-DISK_USAGE.ps1 и т.п.) и запускает его с указанными параметрами.
Скрипт выполняет проверку на локальном узле (например, посылает ICMP-запрос, измеряет свободное место на диске, проверяет статус службы или выполняет SQL-запрос – в зависимости от типа).
Результат выполнения возвращается агенту в стандартизованном формате: включает флаг доступности ресурса (IsAvailable), флаг успешности проверки (CheckSuccess, если есть критерий успешности), метку времени исполнения (Timestamp), подробные данные (Details – например, процент свободного места, время отклика или сообщение об ошибке) и сообщение об ошибке (ErrorMessage, если проверка не была выполнена или ресурс недоступен).
Агент может учитывать interval_seconds задания: если задание предназначено выполняться реже, чем цикл опроса, он может пропускать выполнение до наступления нужного интервала. (По умолчанию используется default_check_interval_seconds, если в задании не указан собственный интервал.)
Отправка результатов: После каждой выполненной проверки Online-агент немедленно отправляет результат на сервер. Для этого выполняется запрос POST к эндпоинту /api/v1/checks с телом JSON, содержащим результат одной проверки. Агент включает в запрос идентификатор задания и полученные данные проверки. Каждый результат передаётся отдельно (по одному заданию за раз). При необходимости агент повторяет попытку отправки (в случае временной недоступности сервера) согласно настройкам или логике на стороне агента.
Логирование: Агент записывает ход выполнения и ошибки в локальный лог-файл (если задан), включая время отправки результатов и любые сбои (например, недоступность API или сбой выполнения конкретной проверки).
Цикл повторяется: Агент засыпает на указанный интервал опроса, затем снова запрашивает задания. Таким образом, при появлении новых заданий или изменении существующих на сервере агент получит их при очередном опросе и начнёт выполнять.
Выходные данные: Online-агент не формирует долговременных локальных выходных файлов (кроме логов). Результатом его работы являются данные, отправленные на центральный сервер. На стороне сервера каждый полученный результат содержит:
assignment_id – ID задания, которому соответствует этот результат (для сопоставления с конфигурацией).
is_available – булево значение, указающее доступность проверяемого узла/ресурса (например, true если узел отвечает на пинг, false если нет).
check_success – булево значение успешности логической проверки (например, true если проверенное значение в пределах нормы; может совпадать с is_available для простых проверок или отражать прохождение критериев).
timestamp – отметка времени, когда проверка выполнялась (в UTC).
details – вложенный объект с подробностями результата (например, процент свободного места, время отклика, полученные данные и т.д. в зависимости от типа проверки).
error_message – текст ошибки, если проверка не выполнена или ресурс недоступен (иначе null).
После получения этих данных серверный компонент запишет их в базу данных. Кроме того, Online-агент может выводить оперативные сообщения в консоль (если запущен вручную) и ведёт локальный журнал, но основной "выход" – это обновление статуса узла в системе. Примеры API-запросов и ответов: Ниже приведены примеры взаимодействия Online-агента с API. Пример 1: Получение списка заданий от сервера. Агент запрашивает у REST API актуальные задания для своего object_id:
GET /api/v1/assignments?object_id=1516 HTTP/1.1
Host: monitor.example.com
X-API-Key: <ключ_агента>

HTTP/1.1 200 OK
Content-Type: application/json

{
  "assignments": [
    {
      "assignment_id": 101,
      "node_name": "Server1",
      "ip_address": "10.0.0.5",
      "method_name": "PING",
      "parameters": {
        "host": "8.8.8.8"
      },
      "interval_seconds": 60,
      "success_criteria": {
        "max_response_time_ms": 100
      }
    },
    ...
  ]
}
В ответ агент получает JSON с массивом заданий. В данном примере показано одно задание: ID 101 для узла "Server1" выполнить проверку PING указанного хоста раз в 60 секунд, ожидая время отклика не более 100 мс. Пример 2: Отправка результата выполнения проверки на сервер. После выполнения задания 101 агент формирует результат и отправляет его:
POST /api/v1/checks HTTP/1.1
Host: monitor.example.com
X-API-Key: <ключ_агента>
Content-Type: application/json

{
  "assignment_id": 101,
  "is_available": true,
  "check_success": true,
  "timestamp": "2025-05-06T07:00:00Z",
  "details": {
    "response_time_ms": 42
  },
  "error_message": null
}

HTTP/1.1 201 Created
Content-Type: application/json

{ "status": "success" }
В запросе приведён результат: узел доступен (is_available: true), проверка успешна по критериям (check_success: true), время отклика 42 ms, ошибок нет. Сервер отвечает кодом 201 (Created) и подтверждает приём результата. После этого запись о проверке будет сохранена в базу данных. Взаимодействие с другими частями: Online-агент напрямую взаимодействует с Backend API (центральным сервером). Он отправляет запросы на чтение заданий и запись результатов. Агент не взаимодействует напрямую с базой данных или веб-интерфейсом – всю обработку его запросов выполняет серверное приложение Flask. Результаты, переданные агентом, сохраняются в PostgreSQL (через вызов хранимой процедуры на сервере) и становятся доступны для отображения в Frontend UI. Таким образом, Online-агент – это "клиент" по отношению к центральному API. Он требует надёжного сетевого соединения с сервером; при недоступности API результаты не будут доставлены (агент должен повторить попытку позже, эти ситуации отображаются в его логах). Все бизнес-правила (какие задания выполнить, как интерпретировать результат) определяются на сервере и в библиотеке StatusMonitorAgentUtils, поэтому поддерживать агент проще – он только исполняет команды и передаёт данные.
2. Offline-агент (PowerShell)
Назначение: Offline-агент предназначен для мониторинга узлов в изолированных сетях, где нет прямого доступа к центральному серверу. Он обеспечивает выполнение тех же проверок, что и онлайн-агент, но офлайн-режиме – т.е. без немедленной отправки результатов. Вместо этого результаты сохраняются локально, чтобы их можно было передать на сервер позже (например, вручную или через промежуточный канал). Offline-агент используется в филиалах или сегментах сети с ограниченной связностью, гарантируя, что мониторинг продолжается даже без постоянного соединения с центральным сервером. Входные данные: Основной вход для офлайн-агента – это локальный JSON-файл конфигурации заданий, который заранее доставляется на машину с агентом. Данный файл (расширение .json.status.*) содержит набор мониторинговых заданий, сгенерированных центральным сервером для данного узла/сети. Он включает:
Версию конфигурации заданий (assignment_config_version) – уникальный тег или хеш, идентифицирующий эту подборку заданий и версию скриптов (используется для отслеживания обновлений).
(Возможно) код транспортной системы (transport_system_code) – условный идентификатор изолированной сети/филиала, для которого предназначен файл.
Необязательные глобальные параметры, например default_check_interval_seconds – интервал по умолчанию между проверками.
Массив assignments – список заданий, каждый элемент которого содержит поля, аналогичные заданиям онлайн-агента: assignment_id, node_name (имя узла или устройства, к которому относится проверка), ip_address (если применимо), method_name (тип проверки), parameters (JSON с параметрами), interval_seconds (интервал запуска) и success_criteria (критерии оценки успеха).
Offline-агент не обращается к API напрямую, поэтому для него критически важен этот конфигурационный файл: он должен быть актуальным и доступным. Доставкой файла занимается компонент Конфигуратор (подробно описан ниже) и физическая транспортировка (например, копирование через накопитель или безопасный канал). Помимо файла заданий, у офлайн-агента есть собственный небольшой конфиг (offline-agent/config.json), где указывается:
object_id или иной идентификатор узла (для идентификации, может использоваться для логов или сопоставления, хотя напрямую к серверу агент не подключается).
assignments_file_path – путь к полученному JSON-файлу с заданиями.
output_path – путь, куда агент должен сохранять результаты (файлы с расширением .zrpu).
check_interval_seconds – рекомендуемый интервал между запусками проверок (если агент работает как служба в цикле; зачастую офлайн-агент запускается периодически планировщиком, поэтому этот параметр может не использоваться или определяет задержку между проверками в рамках одного запуска).
Параметры логирования: logFile, LogLevel и т.п.
Обработка: Работу офлайн-агента можно описать как цикл выполнения заданий с последующим сохранением результатов:
Инициализация: При запуске Offline-агент загружает настройки из offline-agent/config.json (пути и др.) и находит файл заданий по пути assignments_file_path. Агент проверяет актуальность файла (при наличии версии он может игнорировать устаревший конфиг, но обычно предполагается, что в папке всегда самая свежая версия). Затем агент импортирует модуль StatusMonitorAgentUtils – общий для агентов набор скриптов проверок.
Чтение заданий: Агент открывает JSON-файл с заданиями (.json.status.) и парсит его содержимое. Он извлекает метаданные (assignment_config_version, default_check_interval_seconds и др.) и получает массив заданий assignments. Если файл или формат некорректны, агент записывает ошибку в лог и прекращает работу (поскольку без заданий ему нечего выполнять).
Выполнение проверок: Для каждого задания из списка офлайн-агент выполняет проверку локально, аналогично онлайн-агенту:
На основе поля method_name (например, "DISK_USAGE", "PING", "SERVICE_STATUS" и т.д.) выбирается соответствующий скрипт проверки из модуля StatusMonitorAgentUtils (например, Check-DISK_USAGE.ps1).
Агент выполняет скрипт, передавая ему указанные параметры (например, диск "C:" и порог свободного места 20%, или имя службы, или адрес для пинга).
После выполнения формируется результат проверки с необходимыми полями: IsAvailable (удалось ли проверить и доступен ли ресурс), CheckSuccess (логический результат с учётом критериев, если применимо), Timestamp (когда выполнялась проверка, обычно текущее время), Details (структурированные данные или сообщение об ошибке) и ErrorMessage (человекочитаемое описание ошибки, если произошла).
Агент не отправляет результат, а временно сохраняет его в памяти (в список результатов текущего запуска). Если для некоторых заданий задан индивидуальный interval_seconds, а агент запускается как длительно работающий процесс, он мог бы ждать нужное время; однако обычно офлайн-агент запускается планировщиком для однократного выполнения всех задач, поэтому все задания из файла выполняются последовательно в одном запуске.
Формирование выходного файла: После выполнения всех (или доступных) заданий офлайн-агент готовит единый файл с результатами. Формат файла результатов – JSON-объект с расширением .zrpu (специальное расширение, указывающее на пакет результатов офлайн-агента). Структура файла результатов включает:
agent_script_version – версия скрипта агента (строка, например "agent_script_v3.1"), чтобы на сервере знать, какой версией кода сформированы данные.
assignment_config_version – версия конфигурации заданий, с которой работал агент (переносится из входного файла; например, "20240520120000_1060_ABC"). Это важно для сопоставления результатов с теми заданиями, которые были актуальны на момент проверки.
results – массив объектов результатов, по одному на каждую выполненную проверку. Каждый объект содержит поля:
assignment_id – ID задания, которому соответствует результат (чтобы сервер знал, к какому заданию привязать этот результат).
IsAvailable – итог о доступности/работоспособности (True/False).
CheckSuccess – индикатор прохождения проверки по критериям (True/False или null, если неприменимо или не удалось выполнить).
Timestamp – время выполнения (ISO 8601 в UTC).
Details – вложенный JSON с деталями (например, для проверки диска: буква диска, % свободного места; для сервиса: описание ошибки "служба не найдена"; для ping: время отклика и т.д.).
ErrorMessage – строка с сообщением об ошибке, если было исключение или недоступность (дублирует некоторую информацию из Details для удобства).
Агент сериализует этот объект в JSON и сохраняет в файл с расширением .zrpu в указанном каталоге output_path. Обычно имя файла включает идентификатор узла или объекта, а также метку версии/времени (например, 20240520_1060_siteABC.zrpu), чтобы его можно было однозначно идентифицировать при сборе.
Завершение работы: После записи файла Offline-агент может выполнить дополнительные действия: например, удалить или архивировать старый файл заданий (если предполагается, что при каждой доставке конфигурации меняется имя файла), либо сигнализировать локально об успешном выполнении (запись в лог "результаты сохранены в ..."). Затем агент завершается. Как правило, офлайн-агент настроен запускаться периодически (например, каждый час или день) через Планировщик Windows или cron, чтобы регулярно выполнять проверки. Администратор может также запустить его вручную при необходимости.
Выходные данные: Offline-агент генерирует файл результатов в формате .zrpu для последующей передачи на сервер. Этот файл – основной выход агента, содержащий все результаты проверок текущей сессии. Кроме того, как и онлайн-агент, офлайн-агент ведёт локальный лог-файл (если настроен), куда записывает ход выполнения (запуск, сколько заданий выполнено, успех/неуспех каждой проверки, итоговый файл результатов или ошибки). Пример содержимого файла .zrpu (JSON-файл с результатами от офлайн-агента):
{
  "agent_script_version": "agent_script_v3.1",
  "assignment_config_version": "20240520120000_1060_ABC",
  "results": [
    {
      "assignment_id": 101,
      "IsAvailable": true,
      "CheckSuccess": true,
      "Timestamp": "2024-05-20T12:05:30Z",
      "Details": {
        "disk_letter": "C",
        "percent_free": 25.5,
        "execution_mode": "local_agent"
      },
      "ErrorMessage": null
    },
    {
      "assignment_id": 102,
      "IsAvailable": false,
      "CheckSuccess": null,
      "Timestamp": "2024-05-20T12:05:35Z",
      "Details": {
        "error": "Служба 'MyService' не найдена."
      },
      "ErrorMessage": "Служба 'MyService' не найдена."
    }
    // ... возможно, другие результаты ...
  ]
}
В этом примере файл сгенерирован версией скрипта v3.1 на основе конфигурации заданий версии "20240520120000_1060_ABC". Приведены два результата:
Задание 101 – проверка диска C: узел доступен, критерии успешно выполнены (например, свободно 25.5% при минимуме 20%), подробности приведены, ошибок нет.
Задание 102 – проверка сервиса: отмечено, что узел/сервис недоступен (IsAvailable: false), CheckSuccess не определён (так как сама проверка не состоялась), и в деталях указано, что искомая служба не найдена (ErrorMessage дублирует эту информацию).
Взаимодействие с другими частями: Offline-агент не взаимодействует напрямую с сервером или базой данных. Его связь с остальной системой осуществляется через файловый обмен:
Конфигуратор (центральный компонент) генерирует JSON-файл с заданиями, предназначенный для данного офлайн-агента, и обеспечивает его доставку (например, администратор копирует файл на машину агента).
Offline-агент читает этот конфиг, выполняет работу и записывает результаты в .zrpu файл.
Далее результаты должны быть переданы обратно на сервер. Этим занимается Загрузчик результатов (описан далее): он забирает .zrpu файлы, созданные офлайн-агентом, и через API отправляет каждое отдельное измерение в систему.
Таким образом, взаимодействие Offline-агента с сервером происходит асинхронно и опосредованно: файлы конфигурации и результаты могут передаваться любым надёжным способом (переносным носителем, периодической синхронизацией через защищённый канал и т.д.). В документации отмечается необходимость настроить эту транспортную систему обмена файлами, чтобы офлайн-мониторинг функционировал (агент сам по себе не умеет отправлять или получать файлы по сети).
Offline-агент использует общий модуль StatusMonitorAgentUtils, то есть логику конкретных проверок он разделяет с онлайн-агентом. Это гарантирует, что форматы результатов и интерпретация критериев一致ны для обоих типов агентов.
После того, как Загрузчик импортирует данные .zrpu в центральную систему, результаты офлайн-агента становятся неотличимы от результатов онлайн-агента: они хранятся в той же БД и отображаются в том же интерфейсе UI.
3. Конфигуратор (скрипт генерации конфигов)
Назначение: Конфигуратор – вспомогательный компонент, предназначенный для подготовки конфигураций заданий для офлайн-агентов. Поскольку офлайн-агенты не могут напрямую запросить свои задания, конфигуратор берёт на себя задачу получить актуальный список заданий с сервера и сохранить его в виде файла, который затем будет передан в изолированную сеть. Конфигуратор обычно запускается администратором вручную или по расписанию, когда необходимо обновить задания для одного или нескольких офлайн-сегментов. Входные данные: Конфигуратор требует доступа к центральному API и информации о том, для каких объектов (узлов/подразделений) надо сгенерировать конфиг. Его собственный конфигурационный файл (configurator/config.json) содержит:
api_base_url – базовый URL API сервера (как у агентов).
api_key – API-ключ с привилегией configurator (специальная роль, дающая право запрашивать офлайн-конфигурации). Этот ключ хранится безопасно на машине, где запускается конфигуратор.
output_path_base – базовый путь, куда сохранять сгенерированные JSON-файлы конфигураций.
delivery_path_base – (опционально) базовый путь для доставки файлов. Может указывать, например, на сетевую папку, USB-носитель или другое место, откуда файл подхватит администратор для переноса в офлайн-сеть. Если этот параметр задан, конфигуратор после генерации файла может копировать его в папку доставки.
subdivision_ids_to_process – список идентификаторов подразделений (или объектов), для которых нужно сгенерировать конфигурации. Здесь обычно перечисляются ID изолированных филиалов/сегментов, соответствующие офлайн-агентам. Если список пуст, может подразумеваться генерация для всех, либо скрипт потребует указать ID вручную при запуске.
Кроме того, конфигуратор может принимать параметры запуска. Например, администратор может запустить скрипт с указанием конкретного object_id, перезаписав тем самым список из конфига, чтобы сгенерировать файл только для одной локации. (В документации упоминается, что типичный сценарий: Администратор запускает Конфигуратор для нужного ObjectId.) Обработка: Конфигуратор выполняет последовательность действий для каждого указанного офлайн-объекта:
Запрос конфигурации у API: Для каждого object_id (например, соответствующего изолированному подразделению или группе узлов) конфигуратор отправляет запрос GET на эндпоинт API: /api/v1/objects/{id}/offline_config. В заголовке указывается API-ключ с ролью configurator. Пример такого запроса:
GET /api/v1/objects/1060/offline_config HTTP/1.1  
Host: monitor.example.com  
X-API-Key: <ключ_конфигуратора>
Сервер аутентифицирует запрос и инициирует генерацию конфигурации: во Flask-приложении вызвается соответствующий метод, который обращается к базе (хранимая процедура generate_offline_config(...)).
Генерация JSON на сервере: В базе данных процедура generate_offline_config формирует JSON с заданиями:
Проверяется, существует ли подразделение/объект с данным ID и назначен ли ему код транспортной системы (для идентификации офлайн-сегмента).
Собираются актуальные задания (assignments) для всех узлов данного подразделения, которые должны выполняться офлайн. Каждое задание включает ID, имя узла, IP, метод проверки, параметры, интервал и критерии успеха (в JSON-формате).
Формируется хеш содержимого или версия конфигурации: если набор заданий изменился с последней генерации, генерируется новый assignment_config_version (как правило, комбинация текущей даты/времени, ID объекта и кода ТС; либо вычисленный SHA-256 хеш assignments). Эта версия записывается в таблицу версий (для истории и привязки результатов).
На выходе процедура возвращает JSON-объект со структурой, включающей поля: assignment_config_version, transport_system_code, default_check_interval_seconds (если определён глобально), и массив assignments с заданиями.
Получение ответа: Конфигуратор получает от API сгенерированный JSON. Если произошла ошибка (напр., неверный ID или отсутствуют задания), API вернёт сообщение об ошибке (например, {"error": "Subdivision not found"}), которое конфигуратор должен зафиксировать. Предполагая успешный ответ, на стороне конфигуратора теперь имеется конфигурация заданий в виде JSON-данных.
Сохранение в файл: Скрипт конфигуратора сохраняет полученный JSON в файл в output_path_base. Имя файла формируется по определённому шаблону, чтобы отразить версию и принадлежность. Как указано в документации, имя может включать:
Метку версии конфигурации (assignment_config_version или её часть). Например: 20240520120000_1060_ABC – где 20240520120000 дата/время, 1060 – ID объекта, ABC – код транспортной системы.
Расширение, указывающее на тип содержимого, например .json.status.<код> – где <код> это условный код сети. Для примера выше, если код транспортной системы = "ABC", файл может называться 20240520120000_1060_ABC.json.status.ABC.
Некоторые реализации используют двойное расширение (например, .json.status) + код, или часть версии также в имени файла перед расширением. Важно, чтобы на стороне офлайн-агента файл распознавался (агент настроен искать *.json.status.*).
Итог: файл с конфигурацией заданий (JSON) сохраняется на диск. Если delivery_path_base отличается от output_path_base, конфигуратор может скопировать файл в каталог доставки (например, сетевой шар, откуда его заберёт человек или система доофисной доставки). Это обеспечивает физический перенос: администратор может взять этот файл и переместить его в изолированную сеть.
Повтор для всех объектов: Если в конфигураторе указано несколько subdivision_ids_to_process, он повторит шаги 1–4 для каждого, генерируя отдельные файлы для каждого филиала/сегмента.
Логирование: Конфигуратор ведёт журнал своей работы – отмечает время запроса, ID обработанных объектов, имена созданных файлов. В случае ошибок (например, отказ в доступе по API или ошибка генерации) соответствующие сообщения фиксируются. Это позволяет администратору увидеть, успешно ли обновлены конфиги.
Выходные данные: Результатом работы конфигуратора являются обновлённые JSON-файлы конфигураций заданий для офлайн-агентов. Каждый такой файл содержит полный список заданий для соответствующего офлайн-узла/подразделения и необходимую служебную информацию (версия, код). Эти файлы должны быть переданы на целевые машины с офлайн-агентами до следующего запуска агентов, чтобы те использовали актуальные данные. Если у конфигуратора настроена директория доставки, новые файлы появятся там (возможно, перезаписывая старые, либо рядом с ними – в зависимости от политики управления версиями; часто старые файлы можно архивировать или удалять после подтверждения обработки). Примеры API-запросов и ответов: Конфигуратор сам является клиентом API. Вот пример ответа, который конфигуратор может получить от сервера (тело ответа на запрос GET /api/v1/objects/1060/offline_config):
{
  "assignment_config_version": "20240520120000_1060_ABC",
  "transport_system_code": "ABC",
  "default_check_interval_seconds": 120,
  "assignments": [
    {
      "assignment_id": 101,
      "node_name": "Server1",
      "ip_address": "10.0.0.5",
      "method_name": "DISK_USAGE",
      "parameters": {
        "disk_letter": "C",
        "min_percent_free": 20
      },
      "interval_seconds": 300,
      "success_criteria": {
        "min_free_percent": 20
      }
    },
    {
      "assignment_id": 102,
      "node_name": "Server2",
      "ip_address": "10.0.1.5",
      "method_name": "SERVICE_STATUS",
      "parameters": {
        "service_name": "MyService"
      },
      "interval_seconds": 600,
      "success_criteria": null
    }
    // ... другие задания, если есть ...
  ]
}
В данном примере для объекта 1060 с кодом "ABC" сервер сгенерировал конфиг версии "20240520120000_1060_ABC". Интервал по умолчанию 120 секунд (если в заданиях не указано иное). Представлены два задания:
№101 для узла Server1 – проверка дискового пространства диска C: с интервалом 300с и требованием минимум 20% свободно.
№102 для Server2 – проверка наличия/статуса службы "MyService" с интервалом 600с (критерии успеха отсутствуют, видимо, само наличие службы = успех).
Конфигуратор сохранит именно такой JSON в файл. Название файла, например, 20240520120000_1060_ABC.json.status.ABC (условно). Взаимодействие с другими частями:
Backend API (Flask) – главный взаимодействующий компонент для конфигуратора. Он принимает запросы от конфигуратора и, через доступ к базе, формирует ответы. Конфигуратор должен иметь корректный API-ключ и обращаться к правильным эндпоинтам; иначе сервер вернёт ошибку (например, 403 Forbidden, если ключ не той роли, что проверяется, либо 401 Unauthorized, если ключ отсутствует/неверен).
PostgreSQL (DB) – конфигуратор косвенно задействует базу через API. В момент запроса offline_config в базе вызывается процедура generate_offline_config, которая читает таблицы заданий, формирует JSON и может создавать/обновлять запись в таблице версий конфигураций. Конфигуратор напрямую с БД не работает, он лишь получает уже готовый JSON.
Offline-агенты – непосредственно с офлайн-агентом конфигуратор не общается, но производимые им файлы являются входными данными для Offline-агентов. Таким образом, между конфигуратором и офлайн-агентом есть связь через файловую систему: конфигуратор складывает файл в условное "место обмена", а агент его оттуда берёт (или админ переносит).
Администратор/Транспортная система – конфигуратор часто запускается вручную администратором, поэтому в процесс вовлечён человек (или расписание). Админ указывает, что нужно сгенерировать, запускает скрипт. Затем он же или автоматизированная доставка перемещает файл. В некоторых случаях конфигуратор может сам выполнять доставку, если, например, папка доставки – это общая сетёвая папка, видимая из изолированного сегмента (через промежуточный носитель).
Конфигуратор упрощает эксплуатацию: администратору не нужно вручную составлять JSON с заданиями – он гарантированно получает консистентный конфиг от центральной системы, включая все новые или изменённые задания. Это снижает риск ошибок при офлайн-обновлениях.
4. Загрузчик результатов (Result Loader, PowerShell)
Назначение: Загрузчик результатов – компонент, ответственный за прием и загрузку результатов, которые были выполнены офлайн-агентами. Его задача – автоматизировать процесс чтения файлов .zrpu, содержащих результаты проверок, и отправки этих результатов на центральный сервер через REST API. Таким образом, загрузчик "мост" от офлайн-среды к онлайн: он берет накопленные офлайн-агентом данные и интегрирует их в систему мониторинга. Загрузчик обычно работает в онлайн-сегменте (там, где доступен API), имея доступ к тому месту, куда попадают файлы от офлайн-сети (например, папка на сервере, куда администратор скопировал .zrpu с флешки). Входные данные: Основной вход – это файлы результатов .zrpu, созданные офлайн-агентами. Загрузчик должен иметь доступ к каталогу, где аккумулируются эти файлы. Его конфигурационный файл result_loader/config.json определяет:
api_base_url – URL API центрального сервера (аналогично агентам и конфигуратору).
api_key – API-ключ с ролью loader (специальные привилегии для загрузки результатов). Ключ выдаётся системой и хранится на машине загрузчика.
check_folder – локальный путь к директории, в которой появляются файлы .zrpu для обработки. Загрузчик будет сканировать эту папку.
log_file – путь к файлу логов загрузчика.
log_level – уровень детализации логирования (Info, Debug и т.д.).
scan_interval_seconds – периодичность сканирования папки (если реализовано как непрерывный сервис).
Настройки по работе с API: api_timeout_sec (таймаут запросов), max_api_retries (количество повторных попыток при неудаче отправки) и retry_delay_sec (задержка между повторными попытками).
Обработка: Загрузчик результатов, как правило, запускается как фоновый сервис или периодическая задача. Его алгоритм:
Инициализация: При старте загрузчик читает config.json, запоминая параметры API и папки. Убеждается, что check_folder доступна (существует) и что у него есть права чтения/удаления в ней.
Сканирование директории: Логика загрузчика может быть либо разовым запуском (обрабатывает все найденные файлы и завершается), либо постоянным сервисом (бесконечный цикл). В последнем случае он, например, каждые scan_interval_seconds сканирует папку. При сканировании он ищет файлы, соответствующие шаблону, например *.zrpu или более специфическому (например, содержащему в имени _OfflineChecks или код подразделения, как указано в документации). Все новые/необработанные файлы заносятся в список для обработки.
Обработка каждого файла результатов:
Загрузчик открывает файл .zrpu и пытается распарсить его содержимое как JSON. Если файл повреждён или невалидный JSON, фиксируется ошибка (в лог) и такой файл может быть перемещён в папку ошибок или пропущен до ручного разбора.
Из корневого объекта JSON извлекаются метаданные: agent_script_version и assignment_config_version. Они понадобятся для отправки, чтобы сервер знал, с какой версией заданий и агента связаны результаты.
Далее извлекается массив results. Загрузчик перебирает каждый объект внутри results.
Валидация полей результата: для каждого результата проверяется наличие ключевых данных:
assignment_id (обязательный – без него нельзя сопоставить результат с заданием в БД),
IsAvailable (обязательный – даже если False, должен присутствовать; показывает доступность),
Timestamp (обязательный – время проверки; иногда может называться check_timestamp – загрузчик при необходимости приведёт ключи к нужному формату).
Также могут присутствовать CheckSuccess, Details, ErrorMessage – они не всегда обязательны, но их тоже следует передать как есть.
Если критичных полей не хватает или формат неверный (например, assignment_id не число), загрузчик пропускает этот конкретный результат, логируя проблему, и переходит к следующему (при этом файл всё равно будет считаться обработанным, даже если внутри есть пропущенные записи, либо может быть помечен как частично ошибочный – в зависимости от политики).
Отправка результатов на сервер: Для каждого корректного результата загрузчик формирует HTTP-запрос POST к эндпоинту /api/v1/checks (или, альтернативно, использует пакетный эндпоинт /api/v1/checks/bulk – см. ниже). В теле запроса отправляется JSON с полями результата. Кроме основных полей (assignment_id, is_available, и т.д.), загрузчик также включает метаданные assignment_config_version и agent_script_version – сервер их примет и передаст в БД (процедура record_check_result учитывает эти версии при записи, что позволяет связать результат с версией конфигурации и агента).
Загрузчик должен аутентифицироваться: он добавляет заголовок X-API-Key: <ключ_loader> в каждый запрос. Роль loader имеет доступ на добавление результатов (POST /checks).
Повторные попытки и ошибки: Загрузчик использует внутреннюю функцию (например, Invoke-ApiRequestWithRetry) для отправки запросов, которая может несколько раз повторять попытку при сетевых или временных ошибках (согласно max_api_retries и retry_delay_sec в конфиге). Если после всех попыток отправка определённого результата не удалась (например, сервер недоступен), обработка файла приостанавливается или помечается как неуспешная – в реализациях возможно либо прекращение обработки этого файла до следующего цикла, либо продолжение (в случае частичного успеха).
Обычно загрузчик стремится отправить все результаты из файла. В альтернативном варианте, реализованном на сервере, есть эндпоинт /api/v1/checks/bulk, принимающий массив результатов одним запросом. Этот эндпоинт разрешён только для роли loader. Загрузчик мог бы сформировать один запрос с полем "results": [ ... ] содержащим все объекты из файла. Сервер в ответ на bulk-запрос может вернуть код 207 Multi-Status, если часть записей не прошла валидацию (т.е. частичный успех). Однако, даже при использовании поштучных запросов, загрузчик в итоге передаст все результаты.
Логирование процесса: Каждая успешная отправка фиксируется (например: "Result for assignment 101 sent successfully"). В случае ошибки от API (например, 4xx из-за неверных данных или 5xx из-за внутренней ошибки), сообщение регистрируется. В зависимости от критичности, загрузчик либо продолжит со следующим результатом, либо отложит обработку текущего файла.
Отправка события о файле: После того, как все результаты из файла .zrpu обработаны (либо хотя бы попытались обработаться), загрузчик формирует специальное событие для системы. Он отправляет запрос POST на эндпоинт /api/v1/events с данными о том, что файл обработан. Обычно в теле указывается тип события, например:
{
  "event_type": "FILE_PROCESSED",
  "description": "Processed results file 20240520120000_1060_ABC.zrpu",
  "timestamp": "2024-05-20T12:10:00Z"
}
(Здесь description может содержать имя файла или ID подразделения, метку версии – чтобы в журнале можно было идентифицировать какой файл был обработан.) Сервер, получив такое событие (требуется API-ключ с ролью loader, как у загрузчика), записывает его в таблицу системных событий. Это позволяет через UI или логи сервера видеть факт получения офлайн-данных.
Перемещение/удаление файла: Завершив обработку файла, загрузчик перемещает исходный .zrpu файл в другую директорию, чтобы избежать повторной обработки. Обычно заведены поддиректории: например, Processed – для успешно обработанных файлов, Error – для тех, где были серьёзные ошибки. Если файл полностью успешно выгружен, он перемещается в Processed. Если были непреодолимые ошибки (например, файл не читался, или все попытки отправить результаты не удались), файл может оставаться в Error для разбора вручную либо для повторной попытки позже. Перемещение делается простым файловым операцией (rename), что атомарно и быстро даже для больших файлов.
Цикл продолжает работу: Если загрузчик работает постоянно, он снова засыпает на scan_interval_seconds, затем повторяет сканирование папки на случай появления новых файлов.
Выходные данные: Загрузчик напрямую не создаёт новых данных для пользователя, но результаты его работы проявляются на центральном сервере:
Записи о проверках в БД: Все результаты проверок из офлайн-сегментов, после работы загрузчика, оказываются записанными в базу данных, как если бы их прислал онлайн-агент. Таким образом, метрики, статусы узлов и история событий обновляются.
События в системе: Загрузчик генерирует событие FILE_PROCESSED (и, потенциально, события об ошибках обработки), которые сохраняются в журнале событий БД. Эти события можно просматривать через UI (раздел "События") для аудита – например, видно, что "20.05.2024 15:10 обработан файл результатов от филиала ABC, версия конфигурации ...".
Файловая система: Результирующие .zrpu файлы убираются из входной папки. В случае полного успеха они лежат в каталоге Processed (что можно считать выходным артефактом, хранящимся для архивных целей). Если были проблемы – в Error (для повторной обработки или анализа). В идеале администратор периодически очищает/архивирует эти папки.
Логи загрузчика: содержат информацию о переданных результатах и возникших ошибках, помогая отладить проблемы (например, если assignment_id не найден в БД, сервер мог вернуть ошибку – это знак, что офлайн-агент использовал старую конфигурацию, и задание 102 уже не актуально на сервере).
Примеры API-запросов и ответов: (1) Добавление результатов (bulk): Загрузчик может отправлять результаты пакетно. Пример пакетного запроса с двумя результатами из файла:
POST /api/v1/checks/bulk HTTP/1.1
Host: monitor.example.com
X-API-Key: <ключ_loader>
Content-Type: application/json

{
  "results": [
    {
      "assignment_id": 101,
      "is_available": true,
      "check_success": true,
      "timestamp": "2024-05-20T12:05:30Z",
      "details": {
        "disk_letter": "C",
        "percent_free": 25.5
      },
      "error_message": null,
      "agent_script_version": "agent_script_v3.1",
      "assignment_config_version": "20240520120000_1060_ABC"
    },
    {
      "assignment_id": 102,
      "is_available": false,
      "check_success": null,
      "timestamp": "2024-05-20T12:05:35Z",
      "details": {
        "error": "Service 'MyService' not found."
      },
      "error_message": "Service 'MyService' not found.",
      "agent_script_version": "agent_script_v3.1",
      "assignment_config_version": "20240520120000_1060_ABC"
    }
  ]
}

HTTP/1.1 200 OK
Content-Type: application/json

{ "status": "success", "processed": 2 }
Здесь в одном запросе отправлены два результата (соответствуют примеру файла выше). Сервер вернул код 200 и сообщил, что 2 записи обработано успешно. Если бы одна из записей не прошла валидацию на сервере, ответ мог быть 207 Multi-Status с информацией, какая запись не принята (роль loader допускает частичные успехи). (2) Регистрация события: После обработки файл считается успешно загруженным, и загрузчик посылает событие:
POST /api/v1/events HTTP/1.1
Host: monitor.example.com
X-API-Key: <ключ_loader>
Content-Type: application/json

{
  "event_type": "FILE_PROCESSED",
  "description": "File 20240520120000_1060_ABC.zrpu processed",
  "timestamp": "2024-05-20T12:10:00Z"
}

HTTP/1.1 201 Created
Content-Type: application/json

{ "status": "logged" }
Сервер создает запись о событии (тип FILE_PROCESSED), и возвращает подтверждение. Теперь это событие появится в UI (с указанием времени и описанием, в т.ч. имени файла). Взаимодействие с другими частями: Загрузчик является связующим звеном между офлайн и онлайн частями:
Он активно взаимодействует с Backend API, используя два вида эндпоинтов: /checks (для отправки результатов) и /events (для событий). От API он получает ответы о приёме данных или ошибках. Если API недоступно, загрузчик будет ожидать и повторять попытки.
Он требует корректной настройки PostgreSQL на стороне сервера: когда загрузчик шлёт результаты, Flask-приложение вызывает процедуру record_check_result для записи. Все присланные данные записываются в соответствующие таблицы (таблицы результатов проверок, обновление статуса задания/узла, запись события). Таким образом, загрузчик опосредованно взаимодействует с БД, вызывая её функции через API.
Offline-агенты – источник данных для загрузчика. Сам загрузчик не контролирует их, но ожидает от них корректных файлов. Если, к примеру, офлайн-агент сгенерировал файл по старой конфигурации, загрузчик всё равно загрузит эти результаты: сервер при записи может обнаружить, что assignment_config_version не совпадает с текущей, но сохранит результат с пометкой версии (для истории).
Администратор – при полностью ручном процессе админ сам может запускать загрузчик (если он не как сервис). Однако чаще загрузчик настроен как служба или крон-задача, которая работает постоянно, так что участие человека не требуется, кроме как перенести .zrpu файлы в нужную папку.
После работы загрузчика, Frontend UI и прочие компоненты видят агрегированную картину: результаты от офлайн-агентов становятся видны наравне с онлайн. Например, если офлайн-узел не пингуется, то после загрузки результата UI покажет его как "недоступен" с соответствующим временем последней проверки.
В случае обнаружения проблем (например, постоянные ошибки загрузки или файлы, попавшие в Error), требуется вмешательство администратора для разбора – возможно, обновления конфигурации или проверки совместимости версий.
5. Backend API (Flask-приложение)
Назначение: Backend API – центральное серверное приложение, реализующее всю бизнес-логику системы Status Monitor. Это Flask-приложение, работающее в Docker-контейнере (в составе стека с БД и веб-сервером), которое выполняет несколько ролей:
Предоставляет RESTful API для интеграции и агентов. Через этот API внешние компоненты (агенты, конфигуратор, загрузчик) могут получать задания, передавать результаты, фиксировать события и т.д.
Реализует серверную логику мониторинга: создание/хранение заданий, обработка поступающих результатов (расчёт статусов, возможно оповещения), генерация конфигураций по запросу, аутентификация и управление правами.
Отдаёт динамический веб-интерфейс (Frontend UI). Flask через шаблоны генерирует HTML-страницы для пользователя (dashboard, страницы управления и проч.), а также предоставляет некоторые API-эндпоинты для AJAX-запросов UI.
Выполняет задачи аутентификации/авторизации: проверяет API-ключи в запросах, ведёт сессии пользователей для доступа в UI, обеспечивает безопасность (ограничение доступа по ролям).
Служит связующим звеном между интерфейсом, агентами и базой данных, инкапсулируя всю логику работы системы.
Входные данные: Backend API принимает различные виды входящих запросов:
HTTP-запросы от агентов и утилит:
Online-агенты посылают GET запросы на /assignments и POST запросы на /checks.
Конфигуратор посылает GET на /objects/{id}/offline_config.
Загрузчик посылает POST на /checks (или /checks/bulk) и /events.
Каждый такой запрос содержит заголовок X-API-Key с ключом, который Flask-приложение проверяет. API-ключы и их роли хранятся в базе (и, возможно, кэшируются). Если ключ валиден и роль соответствует требуемой для данного эндпоинта, запрос допускается к дальнейшей обработке, иначе возвращается ошибка (401 или 403).
Кроме ключа, запросы содержат данные (JSON в теле или параметры URL), которые приложение валидирует (например, проверяет наличие обязательных полей, правильность типов).
HTTP-запросы от веб-браузера (пользовательский UI):
GET-запросы к веб-страницам (напр. /dashboard, /login, /manage_assignments и т.д.). Flask распознаёт пути и, при необходимости, проверяет, залогинен ли пользователь (сессия). Если нет – переадресует на страницу входа.
POST-запросы от форм UI (например, добавление нового узла, задания или API-ключа через веб-форму). Эти запросы содержат данные форм (может обрабатываться как JSON или как форм-urlencoded, в зависимости от реализации). Приложение проверяет права (только авторизованные пользователи, возможно с определённой ролью "admin", могут создавать/удалять сущности).
AJAX запросы: отдельные страницы могут обращаться к API-эндпоинтам (например, UI может подгружать таблицу событий через /api/v1/events GET). Эти запросы тоже проходят авторизацию (по сессии пользователя либо по тому же механизму API-ключей, если UI использовать API-ключ – но обычно UI использует сессию).
Внутренние запросы/задачи:
Flask приложение может получать сигналы или вызывать фоновые задачи (например, по расписанию очистка старых данных, если реализовано; или оповещение через SocketIO). Они не приходят "снаружи", но являются входными событиями для определённых обработчиков (например, result API может эмитить событие через SocketIO).
Health-check запросы (например, Docker может дергать /health URL). Это простой GET без авторизации, на который приложение отвечает "OK" если работает и БД доступна.
Обработка: Различные части Backend API обрабатывают входящие запросы в зависимости от их назначения:
API для агентов/утилит: Эти маршруты обычно префиксированы, например, /api/v1/.... Приложение настроено (например, Blueprint api_v1) ловить такие URL. Для них, как правило, написаны функции-обработчики с декораторами @api_key_required(role=...). Процесс:
Проверяется наличие заголовка X-API-Key. Если нет – возвращается 401.
Проверяется валидность ключа: приложение ищет его в базе (или кэше) среди выданных API-ключей. Ключи привязаны к ролям (agent, loader, configurator, admin и т.п.). Если ключ не найден – 401 Unauthorized.
Проверяется, соответствует ли роль ключа требуемой для данного эндпоинта. Например, для GET /assignments требуется role=agent; если ключ имеет роль loader или configurator – будет 403 Forbidden. Это предотвращает нецелевой доступ.
Если аутентификация пройдена, функция-обработчик извлекает и валидирует параметры запроса. Например, для /assignments обязателен параметр object_id (ID узла). Если отсутствует или не число – возвращается 400 Bad Request с сообщением о неверном параметре. Для POST /checks проверяется тело: наличие assignment_id, is_available, timestamp и т.д. Если чего-то критичного нет – 400 с описанием.
Выполнение бизнес-логики:
Получение заданий (Online): приложение обращается к базе за списком заданий. Обычно это вызов функции (например, get_active_assignments_for_object(object_id)) или выполнение SQL SELECT из таблицы назначений. Полученные задания форматируются в JSON (список словарей). Приложение возвращает их клиенту с кодом 200.
Генерация офлайн-конфига: приложение вызывает процедуру generate_offline_config(object_id) в базе. База возвращает JSON (как JSONB). Flask-приложение может просто передать его напрямую как ответ (с кодом 200). Либо, если процедура вернула ошибку (JSON с "error"), приложение тоже отдаст это с соответствующим статусом (например, 404, если подразделение не найдено, или 500, если иная ошибка).
Приём результата проверки: приложение получает от агента/загрузчика JSON с результатом. Далее оно вызывает хранимую процедуру record_check_result(...) в базе, передавая туда все необходимые поля: assignment_id, is_available, p_check_timestamp, p_executor_object_id (может браться из ключа или передаваться, хотя в текущей схеме offline-агент присылает объект косвенно через assignment id), assignment_config_version и agent_script_version и т.д. Процедура атомарно вставляет запись о результате в БД, обновляет статус задания/узла. Если процедура отработала без ошибок – контроллер возвращает 201 Created или 200 OK. Если возникла ошибка (например, assignment_id не существует) – может вернуть 400 (с сообщением, что задание не найдено или неактивно).
Пакетный приём результатов: для /checks/bulk приложение итеративно вызывает вышеупомянутую процедуру для каждого элемента массива. Оно собирает информацию об успехе/неуспехе каждого и возвращает сводный статус. Например, если все результаты успешно записаны – 200 OK с { "status": "success", "processed": N }. Если некоторые имели ошибки валидации – может вернуть 207 Multi-Status и в теле перечислить, какие элементы не приняты (стандарт WebDAV Multi-Status, как указывают тесты). Это позволяет загрузчику понять, что файл обработан, но пару результатов не засчитано.
Добавление события: контроллер проверяет роль (только loader может POST /events), потом вставляет новую запись в таблицу events в базе (через INSERT или функцию). После успеха – возвращает 201 Created.
Эндпоинты для UI (JSON-данные): например, /api/v1/dashboard – может агрегировать общее число узлов, число недоступных, процент доступности и вернуть объект с этими цифрами. Он делает SELECT из БД (возможно, вызывает функцию-агрегатор), формирует JSON и возвращает. /api/v1/status_detailed – скорее всего возвращает список всех узлов и их статусов/последних проверок. Это может основываться на функции get_processed_node_status() – которая собирает базовую инфу по узлам и определяет status_class и status_text (как мы видели в коде). Flask вызывает эту функцию и выдает массив узлов с состояниями.
Управление заданиями и пр. через API: возможно, есть эндпоинты для CRUD операций (создать задание, обновить узел и т.д.) для AJAX-части UI. Они проверяют пользователя (либо через @login_required, либо особый ключ admin) и затем выполняют нужные действия (INSERT/UPDATE в БД). Вопрос не фокусируется на них, поэтому глубоко не описываем.
Формирование ответа: после выполнения логики Flask-контроллер отправляет HTTP-ответ с соответствующим статусом. В случае успешных GET – 200 OK + JSON. В случае успешных POST (создание ресурса) – часто 201 Created + JSON {status:"success"} или сам созданный ресурс. При ошибках аутентификации – 401/403, при валидаторных ошибках входных данных – 400 (с описанием всех ошибок, например JSON с полем details), при внутренних ошибках – 500 (и логирует их).
Обновление в реальном времени: Если результат проверки записан, приложение может дополнительно сообщить о новом состоянии через механизм SocketIO. В коде add_check_v1 мы видели, что если расширение SocketIO подключено, то после записи вызова record_check_result определяется node_id_for_socket и через socketio.emit можно уведомить клиентские веб-интерфейсы о новом статусе узла. Это позволяет UI обновлять данные без перезагрузки (речь о веб-сокетах).
Обработка веб-страниц (UI): Flask также содержит набор маршрутов для HTML-страниц (шаблоны). Например, @app.route('/dashboard') (или Blueprint html_routes). Когда пользователь запрашивает страницу:
Если требуется вход (декоратор @login_required), а пользователь не вошёл – перенаправляется на /login.
Если доступ разрешён, функция-обработчик собирает данные для страницы. Например, для dashboard: делает SQL запрос(ы) – либо напрямую через SQLAlchemy/psycopg2, либо через вызовы сервисных функций (как get_processed_node_status() для подготовки статусов).
Полученные данные (список узлов, статистика и т.п.) помещаются в контекст шаблона (словарь).
Затем вызывается render_template('dashboard.html', **data) – Flask подставляет данные в HTML-шаблон.
Готовая HTML страница возвращается браузеру с кодом 200. Браузер её отображает.
Страницы управления (Manage) работают схожим образом: например, при GET запросе на /manage_assignments приложение достаёт из БД список всех заданий (с JOIN-ами, paging и фильтрами, если есть), передаёт в шаблон. Для формы создания нового задания может потребоваться данные справочников (список узлов, список типов проверок и т.д. – их тоже можно извлечь из БД).
Когда пользователь отправляет форму (POST), например создание задания, контроллер считывает поля из request.form или request.json, проверяет валидность (что выбраны узел, метод, параметры корректны JSON-формата, и т.п.), затем вызывает репозиторий/функцию, которая вставляет новую запись в таблицу заданий (и связанные таблицы). По успеху может либо вернуть JSON (если это AJAX) либо сделать redirect на страницу списка с флеш-сообщением "задание создано".
Авторизация: UI может иметь разграничение ролей, например, только администратор может видеть страницы управления API-ключами. Это можно реализовать проверкой флага в сессии или более гранулярной системой ролей.
Выходные данные: Backend API как сервер выдаёт несколько видов результатов:
JSON-ответы для программных клиентов (агентов, утилит, AJAX). В них содержатся либо запрошенные данные (задания, статусы, события), либо подтверждение/результат выполнения (например, статус успеха, созданный объект, ошибки валидации).
HTML-страницы для веб-браузера, которые отображают текущее состояние системы (сгенерированные на основе данных из БД). Это визуальный выход для пользователей.
Сторонние эффекты/действия:
Запись в PostgreSQL – практически каждый запрос API либо читает, либо изменяет данные в базе. Например, REST-запросы агентов приводят к вызовам процедур record_check_result (запись результатов) или generate_offline_config (чтение и потенциально запись версии). Запросы UI – к SELECT/INSERT/UPDATE. Таким образом, одним из основных "выходов" работы backend является модификация состояния базы (создание новых заданий, обновление статуса узлов, сохранение результатов, логирование событий).
Уведомления в UI – при обновлении статуса узла backend может отправить событие через WebSocket, чтобы на клиенте мгновенно изменился индикатор без перезагрузки страницы.
HTTP-статусы – сигнализируют вызывающим сторонам о результате (это важно для агентов: например, 200 OK означает "задание получено", 201 означает "результат принят", 4xx – "ошибка, предприми действие").
Логи сервера – Flask-приложение пишет лог (например, в stdout или файл) обо всех запросах, ошибках, DB исключениях. Это нужно админам для отладки, но не для конечного пользователя.
Примеры API-запросов: (Для иллюстрации, основные эндпоинты системы и их формат)
GET /api/v1/health – проверка доступности сервера. Ответ: {"status": "ok"} (и код 200), если приложение работает и может подключиться к БД.
GET /api/v1/dashboard – получить агрегированные данные для главной страницы. Пример ответа: {"total_nodes": 20, "nodes_up": 18, "nodes_down": 2, "last_update": "2025-05-06T08:00:00Z"}.
GET /api/v1/status_detailed – получить список всех узлов и их статусов. Пример ответа:
{
  "nodes": [
    {
      "node_id": 5,
      "name": "Server1",
      "subdivision": "Office A",
      "type": "Server",
      "status_class": "available",
      "status_text": "В норме",
      "last_check": "2025-05-06T08:00:00Z"
    },
    ...
  ]
}
(Сервер выбирает последние результаты, определяет статус_class = available/unavailable/unknown и соответствующий текст.)
GET /api/v1/events?limit=50 – получить последние 50 событий системы (например, для журнала). Ответ: массив объектов событий (тип, описание, время, источник и т.п.).
POST /api/v1/assignments (предполагаемый эндпоинт создания задания через AJAX) – вход: JSON с описанием задания (узел, метод, параметры, интервал). Ответ: {"status": "success", "assignment_id": 123} (и код 201), если создано.
Взаимодействие с другими частями: Backend API – центральный узел, с которым взаимодействуют все остальные компоненты:
Все агенты и утилиты (Online Agent, Offline Result Loader, Configurator) общаются исключительно с API. Они посылают запросы, а API отвечает. Таким образом, Backend API должен быть постоянно доступен по сети для онлайн-компонентов. Nginx проксирует их запросы к Flask.
PostgreSQL (DB): Flask-приложение постоянно обменивается данными с БД. Для этого при старте приложение устанавливает соединение (или берет из пула), а обработчики запросов выполняют SQL через курсоры или ORM. Критичные операции (запись результатов, генерация конфигов) вынесены в хранимые процедуры на уровне БД, чтобы обеспечить атомарность и сложную логику там, где это удобнее. Например, record_check_result в БД одновременно:
пишет результат в таблицу истории,
обновляет статус последней проверки у задания (может менять флаг "активно/неактивно" или время последнего исполнения),
вычисляет ID узла через join задания, возможно обновляет агрегированный статус узла (например, поле last_seen_available).
После выполнения, процедура может возвращать что-то или нет – API доверяет, что БД всё сделала. Если происходит ошибка (например, нарушение constraints) – БД генерирует исключение, Flask ловит и возвращает 500 или 400.
Процедура generate_offline_config читает задания, рассчитывает версию (с помощью SQL-функций, например digest() для хеша), вставляет новую запись версии (offline_config_versions) при необходимости и возвращает JSON. Это сложная логика, надежно реализована на стороне SQL.
Frontend UI: Само UI частично генерируется внутри Flask (рендер HTML), но фактически UI – отдельный компонент (см. ниже) с точки зрения разбиения. Взаимодействие: UI делает запросы к Flask (как обычный клиент) и получает HTML/JSON. Flask (Backend) аутентифицирует пользователя (сверяется с записями Users в БД, устанавливает сессию). После этого пользовательские действия (нажатия кнопок, отправка форм) преобразуются во входящие запросы (к Backend), которые меняют состояние системы (в БД) и затем Backend выдает новый контент UI. Также Backend может инициировать связь с UI через WebSocket (но технически это тоже клиент-сервер модель: UI подписывается на сокет, Backend публикует события).
Внешний веб-сервер (Nginx): В продакшене Flask-приложение обычно работает behind Nginx. Nginx принимает все HTTP-запросы. Запросы к API и динамическим страницам проксируются на Flask (gunicorn или uWSGI). Статические файлы (CSS, JS, картинки) Nginx отдает сам из директории /static, разгружая приложение. Изнутри система, конечно, это прозрачно – взаимодействие Nginx-Backend лишь инфраструктурное.
Администратор: взаимодействует с Backend через UI (веб-интерфейс) или через CLI-инструменты. Например, чтобы создать нового пользователя или API-ключ, может использоваться flask CLI (как указано: flask create-user ... внутри контейнера). Это тоже форма взаимодействия (через командную оболочку, минуя HTTP), которая вызывает соответствующие функции приложения.
Backend API обеспечивает консистентность: любые изменения через UI или API проходят через единый код, который проверяет правила (нельзя, например, создать дублирующийся ключ или добавить задание на несуществующий узел – Backend/DB отследит и отвергнет). Таким образом, Backend – сердце системы, от чьей корректности зависит работа всех остальных компонентов.
6. Frontend UI (HTML/Flask Templates)
Назначение: Frontend UI – это веб-интерфейс системы Status Monitor, предназначенный для интерактивной работы пользователя (администратора или оператора мониторинга). Он предоставляет наглядное отображение статуса контролируемых узлов и удобные средства управления конфигурацией мониторинга. Основные функции UI:
Отображение текущего состояния системы: список узлов, их доступность (в реальном времени или почти реальном), сводные показатели.
Просмотр деталей: последние результаты проверок, история событий, информация по конкретному узлу или заданию.
Управление мониторингом: через UI можно добавлять/удалять/редактировать подразделения (группы узлов, например филиалы), узлы (конкретные объекты мониторинга), типы узлов (категории с шаблонными свойствами), методы проверки (если это настраивается) и задания (привязка метода к узлу с параметрами). Также UI предоставляет интерфейс для управления API-ключами и, возможно, пользователями системы (создание новых учетных записей, если предусмотрено).
Аутентификация и разграничение доступа: UI включает страницы входа (login) и выхода (logout). Только авторизованные пользователи могут получать доступ к разделам управления. Неавторизованные могут видеть либо страницу логина, либо, возможно, ограниченную сводку (но обычно всё закрыто за логином, кроме, может быть, health).
Обеспечение удобства работы: интерфейс сгруппирован по разделам, имеет фильтры, группы, поиск, чтобы администратор мог быстро найти нужный узел или задание. Также, UI может обновлять информацию динамически (через AJAX или WebSockets), чтобы оператор видел изменения статуса без перезагрузки страницы.
Входные данные: С точки зрения Frontend UI, входные данные – это действия пользователя и данные, получаемые с сервера:
HTTP-запросы пользователя: Когда пользователь вводит адрес в браузере, кликает ссылку или отправляет форму, браузер делает соответствующий GET или POST запрос к Flask-приложению. Например, запрос GET /dashboard происходит при переходе на главную страницу, POST /login – при отправке формы входа. Эти запросы содержат либо параметры URL (при переходах с номером страницы, фильтрами), либо тело (при отправке форм).
Данные из БД (полученные через Backend): При генерации страницы UI, Flask-код извлекает из базы соответствующие данные (список узлов, список событий, и т.д.). Эти данные являются "входом" для шаблона.
Сессия пользователя: После успешного логина в сессии (хранящейся как cookie или серверная сессия id) сохраняется информация о пользователе (его имя, права и т.д.). При последующих запросах UI Flask использует эту сессию, чтобы знать, кто пользователь и разрешено ли ему запрашиваемое действие. Таким образом, сессия – тоже вход (неявный) для UI контроллеров.
AJAX-запросы внутри страниц: Некоторые части UI могут загружаться асинхронно. Например, раздел "События" может при загрузке страницы запросить JSON списка событий через AJAX (к эндпоинту /api/v1/events?limit=100). В этом случае входом для дальнейшего отображения является JSON, полученный от API, и JavaScript код на странице отрисовывает таблицу.
WebSocket сообщения: Если реализовано обновление статуса в реальном времени, браузер при открытии dashboard подключается к WebSocket (например, SocketIO namespace). Когда сервер через Backend API шлет событие (например, изменение статуса узла), браузер получает сообщение (например, {node_id:5, new_status:"unavailable"}) и UI-скрипт обновляет соответствующий элемент (подсвечивает узел как недоступный). Такие сообщения – ещё один вид входных данных для UI, позволяющий реагировать на изменения без пользовательских действий.
Обработка: Frontend UI, по сути, обрабатывается на стороне сервера при генерации, а на стороне клиента при отображении:
Генерация страниц (серверная часть): Контроллеры Flask собирают данные из БД, затем передают их в шаблоны. Шаблоны – это HTML-файлы с вкраплениями Jinja2. Пример обработки:
Шаблон dashboard.html может содержать код:
<h1>Состояние узлов</h1>
<p>Всего узлов: {{ total_nodes }}, Недоступных: {{ nodes_down }}</p>
<table>
  {% for node in nodes %}
    <tr class="{{ node.status_class }}">
      <td>{{ node.subdivision_name }} – {{ node.name }}</td>
      <td>{{ node.status_text }}</td>
      <td>{{ node.last_check|datetime_format }}</td>
    </tr>
  {% endfor %}
</table>
Здесь видно: данные, подготовленные контроллером (total_nodes, nodes_down, список nodes с полями), подставляются. Статус узлов влияет на CSS класс строки (status_class может быть "available" = зеленый фон, "unavailable" = красный и т.д.), status_text – отображаемый текст статуса ("В норме", "Недоступен", "Неизвестно"). last_check форматируется фильтром.
После подстановки шаблона пользователь получает статический HTML.
Шаблоны разделов управления, например manage_assignments.html, скорее всего, формируют таблицу заданий с кнопками редактирования/удаления. Они тоже получают список заданий. Возможен разбиение на страницы (paging) – тогда контроллер передаст current_page, total_pages, etc., а шаблон нарисует элементы навигации.
Формы: Шаблон может содержать HTML-форму:
<form method="POST" action="/manage_assignments">
  <!-- поля формы: выпадающий список узлов, выпадающий список методов, поля параметров (JSON), интервал, критерии -->
  <button type="submit">Создать задание</button>
</form>
Контроллер, отрисовывая форму, может заранее вложить в неё необходимые данные, например, заполнить <select> опциями узлов. Это делается либо прямо в шаблоне (бегло перебрать nodes и вставить <option value="{{ node.id }}">{{ node.name }}</option>), либо отдать JS, который подтянет список.
Навигация: На каждой странице, вероятно, есть меню (например, "Сводка", "Детальный статус", "События", "Управление"). В базовом шаблоне base.html меню прописано, а активный раздел подсвечивается (например, через переменную active_page).
Обработка действий пользователя (серверная):
При нажатии на ссылку – просто GET другой страницы (уже описано).
При отправке формы (POST): Контроллер соответствующей страницы (или отдельный маршрут) выполняет:
Проверку сессии (пользователь авторизован? имеет ли право на действие? – напр. только admin может удалить узел).
Считывание данных формы из request.form (Flask предоставляет словарь полей).
Валидацию данных: например, имя узла не пустое, IP имеет валидный формат, JSON параметров корректен (иногда без сложной проверки, просто сохранится строка).
Выполнение операции: обращение к БД (через вызов репозитория или прямой SQL). Например, создать новую запись в таблице узлов, или обновить задание, или удалить API-ключ.
Формирование ответа: часто после POST делают redirect (PRG – Post Redirect Get) на ту же страницу списка, чтобы пользователь не повторил форму при F5. При редиректе можно установить flash-сообщение (Flask flash) типа "Задание добавлено". Шаблон списка проверит наличие get_flashed_messages() и покажет это уведомление.
Если произошла ошибка (например, такой узел уже есть), контроллер может либо также редиректнуть с flash "Ошибка: узел существует", либо (что удобнее) повторно отобразить форму с уже введёнными данными и сообщением об ошибке. Для этого можно сохранять введённые значения и передавать их шаблону, а также указать список ошибок.
UI, как правило, не показывает "стек ошибок" – он транслирует их в понятные сообщения.
Клиентская логика (в браузере): Помимо отображения полученного HTML, UI может включать JavaScript для улучшения взаимодействия:
Пример: на странице "Управление узлами" список узлов сгруппирован по подразделениям. Возможен скрипт, который автоматически группирует опции или позволяет сворачивать/разворачивать группы.
Фильтры и поиск: JS может фильтровать таблицу на лету без обращения к серверу (если все данные уже загружены).
AJAX обновление: Раздел "События" может иметь фильтры по типу или автообновление каждую минуту. JS код может запрашивать /api/v1/events и добавлять новые строки в таблицу.
WebSocket: Если подключено SocketIO, на странице dashboard JS устанавливает соединение. При получении сообщения о новом результате, JS находит в DOM соответствующую строку узла и меняет класс/текст статуса. Это обеспечивает практически моментальное отражение изменения статуса (например, сервер упал – агент отправил, Backend записал и отправил по сокету – на странице статус узла из "В норме" сменился на "Недоступен (PING)" красным цветом).
Форматы: может использоваться библиотеки (jQuery, Bootstrap, etc.) – судя по упоминанию CSS/JS/иконок статических, UI вероятно использует готовый стиль. Динамика может быть минимальной (вполне возможно, что без AJAX тоже можно работать, просто обновляя страницу).
Мультиязычность/локализация: исходя из текста (русский), UI, вероятно, ориентирован на одну локаль (русский). Но потенцильно Flask поддерживает gettext для шаблонов.
Выходные данные:
Отображаемые страницы: Пользователь видит в браузере отрендеренный HTML – основной выход фронтенда. На ключевых страницах:
Сводка (Dashboard): общее количество узлов, сколько из них недоступны; возможно диаграммы или крупные индикаторы, например "Все системы в норме" или "Есть проблемы". Это первая страница после входа, дающая общее состояние.
Детальный статус: таблица всех узлов, сгруппированных по подразделениям/типам. Каждый узел – строка с именем, типом, текущим статусом (на основе самой важной проверки, возможно PING или совокупности). Статус может обозначаться цветом (зелёный, красный, серый). Можно кликнуть на узел, чтобы увидеть подробности (возможно, переход на отдельную страницу узла).
Страница узла: (предполагается, может быть status_detailed.html или отдельная) – показывает все проверки (задания) для данного узла, их последние результаты, время последней проверки, историю последних N проверок (например, график или список). Позволяет вручную запустить проверку? (Опционально, не указано, но иногда бывает "Run now").
События: список системных событий – сюда относятся: офлайн-файлы обработаны, API-ключ создан/удалён, пользователь вошёл, узел добавлен и т.д., в хронологическом порядке. Есть фильтры по типу события, поиску по ключевым словам, ограничение по дате.
Управление: набор подстраниц:
Подразделения (Sites): список филиалов/групп, с возможностью добавить/удалить. Поля: название, код транспортной системы, родитель (для иерархии).
Узлы (Nodes): список всех узлов с указанием подразделения, типа, IP/адреса. Можно редактировать (сменить название, тип, привязку), добавить новый узел (в форму, вероятно, нужно указать подразделение, тип, IP, имя).
Типы узлов (Node Types): например, "Сервер", "Маршрутизатор", "Рабочая станция". Каждый тип может иметь свойства по умолчанию (например, значок или таймаут Ping). Их тоже можно создавать/редактировать.
Задания (Assignments): список всех заданий (в т.ч. активные и отключенные). Показывает: узел, метод (например, Ping), параметры (порог), расписание (интервал или cron), последнее выполнение, статус. Можно фильтровать по узлу или методу. В UI должна быть возможность добавить задание: выбрать узел, метод, задать параметры, критерии успеха, интервал. После создания запись попадёт в таблицу и (для онлайн-узлов) агент возьмет её при следующем опросе, а для офлайн – будет учитываться при генерации конфигов. Также, вероятно, можно отключить или удалить задание. Отключение может отражаться флагом (и агент его игнорирует).
API-ключи (API Keys): список существующих ключей с описанием и ролями. Здесь можно сгенерировать новый ключ (роль: agent, loader, configurator, admin – для UI?), скопировать его (будет показан один раз), отозвать/удалить старый. Это важно для интеграций: например, добавить новый агент – сначала через UI создают API-ключ с ролью agent и выдают его тому, кто устанавливает агент.
(Пользователи: если реализовано управление пользователями UI, то аналогично – создать/удалить учётку, задать пароль. Возможно, в простой реализации пользователи хранятся статически или создаются только через CLI.)
Интерактивный отклик: UI может выдавать всплывающие уведомления (успех или ошибка) после действий. Например, "Узел успешно добавлен" или "Ошибка: узел с таким именем уже существует". Это также часть пользовательского вывода (feedback).
Статические файлы: UI включает CSS (таблица стилей для оформления) и JavaScript (для интерактивности). Эти файлы хранятся и отдаются Nginx-ом, но они являются частью фронтенда. В частности, CSS определяет внешний вид (цвета статусов, оформление таблиц и форм), а JS – поведение (например, сортировка таблицы, подтверждение перед удалением и т.п.). Пользователь не видит их отдельно, но они влияют на его опыт.
Взаимодействие с другими частями:
Backend API (Flask): UI полностью зависим от backend-приложения, так как именно оно генерирует страницы и предоставляет данные. Любое действие в UI – это запрос к Backend. Если Backend не доступен, UI не сможет загрузиться (покажет ошибку соединения). Внутри одного Flask-приложения фронтенд и API – это просто разные маршруты. Они могут совместно использовать одни и те же функции доступа к БД (например, UI и API могут пользоваться единым сервис-слоем).
PostgreSQL: UI косвенно взаимодействует с базой через Backend. Например, когда UI отображает список узлов, Backend получил их из БД за секунду до отдачи страницы. Когда пользователь жмет "удалить узел", Backend выполняет DELETE в БД.
Агенты и Загрузчик: UI получает от них информацию через базу. Например, UI показывает, что "последняя проверка ПИНГ на узле X была в 12:00 и успешна" – эти данные появились, потому что в 12:00 онлайн-агент отправил результат, backend записал его, а UI запросил БД. Если агенты не будут отправлять данные, UI просто будет показывать устаревшие метки времени или недоступность. Также, UI может инициировать (косвенно) агентов: например, администратор через UI добавил новое задание – в БД оно появилось с активным статусом; онлайн-агент через минуту его получит и начнёт выполнять. Если UI-пользователь отключит задание (снимет флаг активность), backend отметит это в БД, и онлайн-агент либо перестанет получать его, либо сам заметит изменение статуса (в зависимости от реализации API).
Конфигуратор/Offline: UI, строго говоря, не напрямую общается с ними, но предоставляет средства управления, влияющие на них:
Когда через UI создаются задания на узлы, которые в офлайн-подразделении, эти задания также сохраняются в БД. Но офлайн-агент о них не узнает, пока конфигуратор не выгрузит новый JSON. Поэтому администратор, добавив задание в UI, должен помнить запустить конфигуратор и доставить файл. В идеале система могла бы показать уведомление: "Не забудьте обновить офлайн-конфигурацию для данного филиала", но это уже тонкости – скорее это на ответственность пользователя.
UI показывает, когда последний раз обновлялась конфигурация офлайн-узла: возможно, через события FILE_PROCESSED и, может быть, хранящийся timestamp версии. Администратор, видя в UI, что у филиала ABC конфигурация заданий старой версии, может нажать кнопку "Обновить офлайн-конфиг" (если бы была). В отсутствие такой кнопки – выполнит это вне UI.
Когда офлайн-результаты загружены, UI в разделе "События" отразит событие "FILE_PROCESSED for ABC", а на страницах узлов этого филиала обновятся данные проверок (их результаты появятся, статусы узлов поменяются). Таким образом, UI – конечный пункт, где администратор убедится, что офлайн-данные вошли (например, видит: "Server2 в филиале ABC: последнее проверено в 12:05, недоступен – служба не найдена").
Безопасность и доступность: UI компонент требует от Backend организовать HTTPS (обычно Nginx SSL termination) для безопасного логина, защиту от CSRF (Flask-WTF при формах может ставить CSRF token), и общую надежность. Пользователи взаимодействуют с UI через браузер – значит, должен поддерживаться и проверяться на современных браузерах.
7. PostgreSQL (база данных)
Назначение: PostgreSQL – система управления базами данных, используемая как хранилище всей информации, связанной с Status Monitor. Она служит надежным источником данных для всего приложения:
Хранит конфигурационные данные: сведения об узлах, структуре подразделений, типах узлов, о том какие проверки на каких узлах должны выполняться (набор заданий), параметры этих проверок, а также пользовательские учетные записи и API-ключи.
Хранит результаты мониторинга: все результаты проверок, поступающие от агентов (возможно с историей), текущее состояние/статус узлов, журнал событий (например, недоступность, восстановление, обработка файлов).
Обеспечивает целостность и согласованность: за счет реляционных связей и транзакций гарантируется, что, например, нельзя создать задание для несуществующего узла, или что результат проверки всегда ссылается на валидное задание. Транзакции используются, чтобы запись результата обновляла нужные поля атомарно.
Выполняет часть логики на своей стороне через хранимые процедуры (functions/procedures). Это делается для повышения производительности и упрощения кода приложения. Например, процедура record_check_result внутри БД может за один вызов обновить несколько таблиц, что на клиенте (Flask) потребовало бы нескольких последовательных запросов и сложной обработки. Хранимые процедуры также уменьшают задержку между связанными операциями.
Обеспечивает долговременное хранение: база данных сохраняет историю проверок и событий, позволяя анализировать тренды, прошлые сбои, статистику (в рамках доступного объема). Возможно, накапливаются тысячи/миллионы записей проверок.
Структура данных (основные сущности в БД):
Подразделения (subdivisions): отражают структуру организации или сети. Поля: id (PK), name (название, например "Офис Санкт-Петербург"), parent_id (если есть иерархия, может быть NULL для корневых), object_id (внешний идентификатор, который используются агентами/config – например, 1060 как во входных данных), transport_system_code (код ТС, применимо для офлайн-подразделений; например "ABC" для обозначения конкретного оторванного сегмента). Эта таблица нужна для группировки узлов и для генерации офлайн-конфигов (как мы видели, generate_offline_config ищет subdivision по object_id).
Узлы (nodes): представляют конечные объекты мониторинга (серверы, устройства, сайты). Поля: id, name, ip_address (или доменное имя), node_type_id (ссылка на тип узла), parent_subdivision_id (ссылка на подразделение, в котором находится узел), возможно properties (JSON с дополнительными свойствами узла). Например, узел "Server1" с IP 10.0.0.5, тип "Windows Server", принадлежит подразделению "Office A". В бизнес-логике node_id часто используется: задания ссылаются на узел, результаты через задания могут получить узел.
Типы узлов (node_types): справочник категорий узлов. Поля: id, name (например, "Server", "Router"), icon (имя иконки или SVG-код), properties_schema (возможно, определение свойств), display_order (для сортировки). Node type может определять, какие проверки обычно применимы, или как группировать.
Методы проверки (check_methods): справочник видов мониторинговых проверок. Поля: id, method_name (например, "PING", "DISK_USAGE", "SERVICE_STATUS", "SQL_QUERY"), description (что делает), default_parameters (JSON шаблон), default_interval (по умолчанию, сек), script_name (например, соответствующее имя скрипта "Check-PING.ps1"). Эта таблица определяет, какие проверки вообще доступны системе. При создании задания указывается ссылкой method_id или сохраняется method_name.
Задания (node_check_assignments): ключевая таблица, связывающая узлы с методами для периодических проверок. Каждая запись – одно мониторинговое задание.
Поля: id (assignment_id), node_id (FK -> nodes), method_id (FK -> check_methods),
parameters (JSON с параметрами конкретно для этого узла; например, для PING может быть адрес хоста (если не использовать ip узла), для DiskUsage – имя диска и порог и т.д.),
check_interval_seconds (периодичность, если отличается от дефолта),
success_criteria (JSON с критериями успешности, например, {"min_free_percent": 20}),
is_active (булево, активно ли задание; отключённые задания не должны выполняться агентами),
last_executed_at (timestamp последнего выполнения),
last_status / last_is_available / last_check_success (сводка последнего результата: доступен или нет, успешность и, возможно, текст ошибки).
created_at, updated_at (временные метки создания/обновления задания).
Эта таблица – основа для выдачи заданий агентам. Для онлайн-агентов выдаются все активные задания для данного node (по его object_id, via join node->subdivision maybe). Для офлайн – generate_offline_config берет все активные задания для всех узлов подразделения.
Результаты проверок (check_results или подобная): таблица исторических результатов. Поля:
id,
assignment_id (FK -> assignments),
timestamp (время выполнения),
is_available (bool),
check_success (bool или NULL),
details (JSON с подробностями результата; может хранить, например, {"response_time": 42} или {"error": "timeout"}),
error_message (текст, дублирующий error из details, для быстрых выборок),
assignment_config_version (версия конфигурации, пришедшая с результатом, если есть),
agent_script_version (версия скрипта агента, приславшего результат),
executor_object_id (опционально, ID объекта, от которого пришел результат, можно вытаскивать из ключа, но скорее хранится отдельно; однако, assignment_id уже достаточно чтобы через join assignment->node->subdivision->object_id узнать).
Возможно, node_id продублирован для удобства.
Эта таблица постоянно растёт – каждое получение результата через /checks добавляет запись. record_check_result заполняет её.
Версии офлайн-конфигураций (offline_config_versions): хранит информацию о выпущенных версиях заданий для офлайн-агентов (чтобы отслеживать, с каким набором заданий пришёл результат). Поля:
version_id (PK),
version_tag (строка, тот самый assignment_config_version как в файлах, уникальная),
object_id (внешний ID подразделения, для которого эта версия, может быть NULL если на весь),
config_type (например, 'assignments' или тип агента),
content_hash (хеш содержимого assignments, используется для определения изменения),
description (описание, напр. "Авто-версия заданий"),
transport_system_code (на момент генерации, дублирует из subdivision),
is_active (булево, актуальная ли версия; генерация новой может пометить старые как неактивные).
Процедура generate_offline_config при генерации нового комплекта заданий создаёт здесь запись с is_active=TRUE, и, возможно, помечает предыдущие версии этого же объекта как FALSE. Благодаря этому, когда офлайн-агент пришлёт результаты с каким-то version_tag, на сервере можно понять – это старая версия (если is_active=FALSE).
Пользователи (users): хранит учетные записи для входа в UI. Поля: id, username, password_hash, role (если роли, например admin/user), is_active.
API-ключи (api_keys): хранит выданные ключи и их роли. Поля: id, key (сам строковый ключ, длинный), role (строка, напр. 'agent', 'loader', 'configurator', 'admin'), description (подпись, чтобы админ знал, чему выдан, напр. "Online Agent Server1"), created_at, last_used_at, is_active.
События (events): журнал разного рода событий. Поля: id, event_type (строка, напр. 'NODE_DOWN', 'NODE_UP', 'FILE_PROCESSED', 'ASSIGNMENT_CREATED', 'USER_LOGIN'), description (текст/JSON с деталями события, например "Node Server1 is down (no ping)" или "Results file ABC.zrpu processed"), timestamp, возможно related_object_id (например, id узла или subdivision, к которому относится событие), severity (info/warning/critical).
Прочее: могут быть таблицы settings (пары ключ-значение глобальных настроек, упоминалось default_check_interval_seconds хранится там), node_properties (если узлы имеют доп. свойства), node_type_properties (шаблонные свойства типов), audit-таблицы.
Обработка (хранимые процедуры):
record_check_result(p_assignment_id, p_is_available, p_check_timestamp, p_check_success, p_error_message, p_details_json, p_agent_version, p_assignment_version, p_executor_object_id) – псевдосигнатура. Внутри:
Проверяет, существует ли задание с таким ID, активно ли оно. Если нет – может бросить ошибку или все равно записать, но пометить особо. Вероятно, если задание не найдено, возвращает SQLSTATE 23503 (FK violation) или специально делает RAISE EXCEPTION 'Assignment not found'.
Находит связанный узел (JOIN assignment->node) и, возможно, связанные данные (method, etc. – method может пригодиться для логики success?).
Вставляет новую строку в check_results: assignment_id, timestamp (приводит p_check_timestamp к timestamptz), is_available, check_success, error_message, details (p_details_json ::jsonb), assignment_config_version = p_assignment_version, agent_script_version = p_agent_version, executor_object_id = p_executor_object_id.
Обновляет node_check_assignments:
last_executed_at = p_check_timestamp,
last_is_available = p_is_available,
last_check_success = p_check_success,
last_error_message = p_error_message (возможно),
возможно, обновляет какие-то агрегаты – например, если метод = PING, может обновить поле last_ping_success или uptime.
Обновляет nodes:
Может сохранять время последнего ответа (например, last_seen = now() if is_available true), либо время последней проверки вообще.
Может обновлять поле summary_status – обобщенный статус узла. Например, если PING не проходит, ставит node.status = "DOWN".
Или, альтернативно, summary_status вычисляется на лету функцией (как get_processed_node_status делает), тогда node не хранит напрямую статус, а хранит только детали.
Если результат означает изменение состояния (например, узел был доступен, стал недоступен), можно сразу создать запись в events типа NODE_DOWN. В коде мы видели упоминание event (в pos 7-10 record_check_result_proc).
Фиксирует изменения (COMMIT). Процедура может быть PROCEDURE (не function), тогда вызывается через CALL, и не возвращает значения, только через OUT параметры или через эффект.
generate_offline_config(p_executor_object_id) – описана ранее:
Проверяет наличие subdivision с object_id = заданному. Если нет – возвращает JSON {"error": "Subdivision not found"}. Если есть, но transport_system_code NULL (не настроен) – возвращает {"error": "Subdivision transport_system_code is missing"} (т.е. нужно задать код, иначе не знает, как назвать файл).
Берёт default_interval из таблицы settings.
Выбирает все задания для узлов этого подразделения (JOIN assignments->node->subdivision) где subdivision.id = найденному. Только активные задания (наверняка is_active = TRUE). Формирует JSON-массив assignments: в каждой записи строит JSON с нужными полями (мы видели в SQL: assignment_id, node_name, ip_address, method_name, parameters, interval_seconds, success_criteria).
Вычисляет хеш (SHA256) от полученного JSON-массива assignments (v_content_hash).
Сравнивает с последним сохраненным hash для этого object (SELECT last.content_hash FROM offline_config_versions WHERE object_id=... AND is_active). Если совпадает – значит задания не изменились с предыдущей генерации:
Можно вернуть старую версию? Но скорее всего нет, обычно даже если совпало, можно вернуть ту же версию. В коде snippet SELECT ocv.version_tag ... вероятно пытается найти, есть ли уже такая версия с тем же hash. Если есть – он берет её version_tag (v_assignment_version_tag).
Если не нашлось или hash изменился – генерирует новый version_tag: 'YYYYMMDDHHMMSS_objectId_transportCode'.
Вставляет новую запись в offline_config_versions (ON CONFLICT DO NOTHING — чтобы если два параллельных, но у нас не параллель).
Предыдущие версии можно пометить is_active=FALSE, но непонятно, делают ли. Возможно, is_active всех, кроме последней, = FALSE.
Формирует итоговый JSON:
{
  "assignment_config_version": "<version_tag>",
  "transport_system_code": "<code>",
  "default_check_interval_seconds": <int>,
  "assignments": [ ... ]
}
и возвращает как JSONB (в PL/pgSQL можно RETURN JSONB).
Коммит.
Другие процедуры/функции:
create_assignments_unified (упоминалось) – вероятно, удобная функция, чтобы создать несколько заданий разом (bulk create) на основе критериев (в UI, возможно, "назначить метод X всем узлам типа Y" – массовое добавление).
get_node_base_info – возвращает объединенную инфу об узлах (как видели, join типов, подразделений).
get_processed_node_status – в коде Python: делает fetch_node_base_info, затем для каждого определяет status_class/text на основе последней проверки Ping. Возможно, вместо хранить node.status, они решают on the fly: if node.last_ping time > threshold -> unknown, if is_available false -> unavailable, else available.
record_event(p_type, p_description, p_object_id, p_severity) – вставляет строку в events.
authenticate_user(username, password) – функция/процедура для входа (может, но обычно это приложение делает, не БД).
Представления (views): может быть view или matview для сводки.
Выходные данные: База сама по себе не "выдаёт" данные конечному пользователю, но:
Она возвращает результаты запросов Backend-у. Например, SELECT запрос возвращает набор строк, который Backend преобразует в JSON или HTML для UI/агентов.
Результаты выполнения процедур:
generate_offline_config возвращает JSON, который Backend просто перенаправляет конфигуратору.
record_check_result возможно не возвращает ничего, либо может возвращать, например, computed status or event. Но, скорее всего, просто через OUT параметр может передать, например, node_id чтобы Flask знал, кому отправить socket уведомление.
Управление данными: БД обеспечивает, что данные корректно сохранены на диск (PostgreSQL – transational, crash-safe). Это важно для durability: выходом любой транзакции, подтвержденной БД, является гарантированно сохраненные изменения (например, новый API-ключ записан, новое задание есть).
Представления для отчетности: если бы были, они бы представляли удобный выход для отчетов (но в этой системе больше оперативные данные, не отчеты).
Примеры SQL-запросов:
Получить активные задания для онлайн-агента:
SELECT id AS assignment_id, node_id, method_id, parameters, check_interval_seconds
FROM node_check_assignments 
WHERE node_id IN (SELECT id FROM nodes WHERE parent_subdivision_id = (SELECT id FROM subdivisions WHERE object_id = 1516))
  AND is_active = TRUE;
(Вместо этого, скорее, используется процедура get_active_assignments_for_object(1516).)
Вставка результата (процедура вызывается, но эквивалент):
INSERT INTO check_results 
  (assignment_id, timestamp, is_available, check_success, details, error_message, assignment_config_version, agent_script_version)
VALUES 
  (101, '2024-05-20 12:05:30+00', TRUE, TRUE, '{"disk_letter":"C","percent_free":25.5}'::jsonb, NULL, '20240520120000_1060_ABC', 'agent_script_v3.1');
UPDATE node_check_assignments 
  SET last_executed_at = '2024-05-20 12:05:30+00', last_is_available = TRUE, last_check_success = TRUE 
  WHERE id = 101;
(В реальности record_check_result делает все вместе.)
Получение статуса узлов для UI:
SELECT n.id, n.name, s.name as subdivision_name, nt.name as type_name,
       nca.last_is_available as is_available, 
       COALESCE(nca.last_check_success, false) as check_ok,
       nca.last_executed_at as last_check
FROM nodes n
JOIN subdivisions s ON n.parent_subdivision_id = s.id
JOIN node_types nt ON n.node_type_id = nt.id
LEFT JOIN node_check_assignments nca ON n.id = nca.node_id AND nca.method_id = (SELECT id FROM check_methods WHERE method_name='PING') 
-- предположим, за обобщенный статус берем последнее по Ping
;
(Здесь логика: берем для каждого узла его последнее состояние ping. Дальше уже Python-код определяет status_class как 'available' если is_available=true, 'unavailable' если is_available=false, 'unknown' если нет данных давно.)
Взаимодействие с другими частями:
Backend API: единственный прямой клиент БД. Все запросы идут отсюда. Они идут через драйвер (psycopg2), внутри Docker-сети (обычно). Если БД недоступна, Backend API не сможет выполнять запросы и будет возвращать ошибки (в /health сразу станет видно).
Миграции/DDL: При обновлениях системы, разработчик применяет SQL-скрипты (например, с помощью alembic) для изменения схемы. Это вне работы самой системы, но важно, чтобы схема соответствовала ожиданиям кода.
Безопасность: БД, вероятно, настроена принимать соединения только от приложения (может аутентификация по пользователю/паролю). В Docker это все локально. Конечные агенты/UI не имеют прямого доступа к БД, поэтому все правила доступа применяет Backend. БД должна быть защищена и резервироваться, чтобы не потерять данные мониторинга.
Производительность: PostgreSQL справляется с одновременными запросами. Возможны ситуации: много агентов шлют результаты одновременно (bulk loader может за раз несколько). Транзакции record_check_result могут идти параллельно, поэтому в ней должны быть предусмотрены блокировки строк (UPDATE assignment, concurrency на разные задания – ok, на одно и то же задание – редкий случай, но если 2 агента отправят один и тот же assignment (не должно быть, assignment уникален для узла, два агента не должны иметь один assignment).
Размеры данных: С течением времени таблица check_results растет. Необходимо, возможно, реализовать очистку устаревших данных (например, старше 1 года) – или через скрипты, или вручную. В ТЗ это не описано, но на практике может понадобиться.
Связь с офлайн: offline_config_versions помогает связать приходящие результаты с конфигурацией, на основе которой они получены. Это важно: например, если результат пришел от задания, которого уже нет (assignment_id удалён), система все равно сохранит результат, но как она его свяжет?
Возможно, assignment записи не удаляют сразу при отключении, а помечают inactive. Тогда и assignment_id останется. Если же удалить, record_check_result может либо игнорировать, либо создавать placeholder. Логичнее, что удалять задания, под которые могут прийти результаты, нежелательно; лучше отключать и подождать, пока не придут старые. Это нюанс эксплуатации, не явно покрытый ТЗ, но вытекает из консистентности.
Резервное копирование и восстановление: БД должна регулярно бэкапиться, т.к. содержит ключевые данные. Это вне рамок разработки, но часть системы эксплуатации.
Таким образом, PostgreSQL – критически важный компонент. Без него ни API, ни UI не смогут ни получить конфигурацию, ни сохранить новые данные. Все взаимодействия между компонентами сходятся на уровне базы: агенты -> API -> DB, UI -> API -> DB, конфигуратор/загрузчик -> API -> DB. Бизнес-правила (активность заданий, актуальность версий, реакция на результаты) реализованы комбинацией кода Flask и логики внутри самой базы.